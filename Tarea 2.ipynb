{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2: análisis de sentimientos usando aprendizaje supervisado\n",
    "\n",
    "##### Por: Daniela Flores Villanueva \n",
    "\n",
    "## Sobre las librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, se cargan todas las librerías empleadas en esta tarea:\n",
    "- `pandas`: librería utilizada para cargar la matriz GloVe y posteriormente, para confeccionar un `DataFrame` con palabras y su sentimiento asociado.\n",
    "- `csv`: utilizada para cargar los vectores, según lo dispuesto en [este](https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python) sitio.\n",
    "- `nltk`: se usó para cargar el *opinion lexicon* descrito en el enunciado de manera conveniente. Se probó además sus métodos para realizar lematización, proceso que consiste en, dada una palabra flexionada (en forma plural, verbo conjugado, etc), encontrar su lema, es decir, algo similar a la forma en que sería encontrada en un diccionario."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /Users/DanielaFlores/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download('opinion_lexicon')\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_PATH = \"./glove.42B.300d.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma de cargar el opinion lexicon es la descrita en la [documentación](http://www.nltk.org/api/nltk.corpus.reader.html?highlight=wordnet) de NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = opinion_lexicon.positive()\n",
    "negative_data = opinion_lexicon.negative()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el fin de tener una representación más general de las palabras, se convierte cada vocablo en las listas anteriores a minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = [word.lower() for word in positive_data]\n",
    "negative_data = [word.lower() for word in negative_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se carga la matriz de *embeddings*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_matrix = pd.read_csv(GLOVE_PATH, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE, na_values=None, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el mismo recurso para cargar la matriz en memoria (adjunto en la descripción de las librerías utilizadas), se menciona una función que, dada una palabra, retorna su *embedding*. Esta función es `vec(w)`, definida a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec(w):\n",
    "    return glove_matrix.loc[w].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breve análisis exploratorio\n",
    "\n",
    "No se puede empezar a trabajar sin tener al menos una noción básica de la composición de la matriz y de las palabras con etiqueta. Por esta razón, en primer lugar cabe preguntarse cuántas palabras hay en la matriz de *embeddings*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1917494\n"
     ]
    }
   ],
   "source": [
    "glove_words = set(glove_matrix.index.tolist())\n",
    "print(len(glove_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podría ocurrir que no todas las palabras que tenemos etiquetadas según su polaridad tengan su representación vectorial en GloVe, por lo que conviene realizar el siguiente análisis:\n",
    "Dada la unión entre las palabras de polaridad positiva y las de polaridad negativa, se revisa si están o no en la matriz. De no estar, se agregan a la lista `missing_words`, para decidir qué hacer con ellas futuramente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n"
     ]
    }
   ],
   "source": [
    "missing_words = set()\n",
    "for word in set(positive_data).union(set(negative_data)):\n",
    "    if word not in glove_words:\n",
    "        missing_words.add(word)\n",
    "print(len(missing_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible notar que hay 161 palabras del *opinion lexicon* para las que no se tiene una representación vectorial. Antes de decidir qué hacer con estas palabras faltantes, conviene preguntarse si existen palabras que estén etiquetadas tanto de polaridad positiva como de polaridad negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'enviousness', 'enviously', 'envious'}\n"
     ]
    }
   ],
   "source": [
    "wrong_tag = set(positive_data).intersection(negative_data)\n",
    "print(wrong_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, se evidencia que tres palabras relacionadas a la envidia parecen estar mal etiquetadas, pues no deberían aparecer entre las palabras positivas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se procede a realizar lematización de los vocablos positivos y negativos, para ver si existe algún cambio en la cantidad palabras sin *embedding* tras este cambio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data_lemmatized = {nltk.stem.WordNetLemmatizer().lemmatize(word) for word in positive_data}\n",
    "negative_data_lemmatized = {nltk.stem.WordNetLemmatizer().lemmatize(word) for word in negative_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n"
     ]
    }
   ],
   "source": [
    "missing_words_lemmatized = set()\n",
    "for word in positive_data_lemmatized.union(set(negative_data_lemmatized)):\n",
    "    if word not in glove_words:\n",
    "        missing_words_lemmatized.add(word)\n",
    "print(len(missing_words_lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'plea', 'enviousness', 'enviously', 'envious'}\n"
     ]
    }
   ],
   "source": [
    "wrong_tag_lemmatized = set(positive_data_lemmatized).intersection(negative_data_lemmatized)\n",
    "print(wrong_tag_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data_lemmatized = positive_data_lemmatized - wrong_tag_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al lematizar, es posible notar que hay una palabra menos que no tiene representación vectorial. En la siguiente celda, podrá evidenciarse qué palabra es la que hace la diferencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spoilages'}\n"
     ]
    }
   ],
   "source": [
    "print(missing_words - missing_words_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de los datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://github.com/stanfordnlp/GloVe/search?utf8=%E2%9C%93&q=unk&type="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_matrix = np.empty((0, 300))\n",
    "for word in positive_data_lemmatized:\n",
    "    if word in glove_words:\n",
    "        positive_matrix = np.append(positive_matrix, [vec(word)], axis=0)\n",
    "    else:\n",
    "        positive_matrix = np.append(positive_matrix, [vec(\"unk\")], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_matrix = np.empty((0, 300))\n",
    "for word in negative_data_lemmatized:\n",
    "    if word in glove_words:\n",
    "        negative_matrix = np.append(negative_matrix, [vec(word)], axis=0)\n",
    "    else:\n",
    "        negative_matrix = np.append(negative_matrix, [vec(\"unk\")], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1951, 300)\n",
      "(4492, 300)\n"
     ]
    }
   ],
   "source": [
    "print(positive_matrix.shape)\n",
    "print(negative_matrix.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_labels = [1] * positive_matrix.shape[0]\n",
    "negative_labels = [0] * negative_matrix.shape[0]\n",
    "all_labels = positive_labels + negative_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6443, 300)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "concatenated_matrix = np.concatenate([positive_matrix, negative_matrix], axis=0)\n",
    "concatenated_matrix.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación\n",
    "\n",
    "### Separación entre entrenamiento y *testing*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(concatenated_matrix, all_labels, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimator_grid_search(estimator, parameters):\n",
    "    clf = GridSearchCV(estimator=estimator, param_grid=parameters, n_jobs=-1, cv=5)\n",
    "    clf.fit(X_train, y_train)   \n",
    "    print(\"Mejor accuracy: {}\".format(clf.best_score_))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(clf):\n",
    "    metrics = {}\n",
    "    predictions = clf.predict(X_test)\n",
    "    metrics[\"accuracy\"] = accuracy_score(y_test, predictions)\n",
    "    metrics[\"precision\"] = precision_score(y_test, predictions)\n",
    "    metrics[\"recall\"] = recall_score(y_test, predictions)\n",
    "    metrics[\"f1-score\"] = f1_score(y_test, predictions)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(clf, clf_name):\n",
    "    joblib.dump(clf, \"best_{}.pkl\".format(clf_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor accuracy: 0.93267365153279\n",
      "Mejor C: 10\n",
      "Mejor kernel: rbf\n",
      "Mejor Gamma: 0.001\n",
      "{'accuracy': 0.9588828549262994, 'precision': 0.9510869565217391, 'recall': 0.9090909090909091, 'f1-score': 0.9296148738379815}\n"
     ]
    }
   ],
   "source": [
    "svm_params = {\n",
    "    \"C\": [1, 10],\n",
    "    \"gamma\": [0.001, 0.0001],\n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "}\n",
    "tuning_svm = estimator_grid_search(SVC(), svm_params)\n",
    "print(\"Mejor C: {}\".format(tuning_svm.best_estimator_.C))\n",
    "print(\"Mejor kernel: {}\".format(tuning_svm.best_estimator_.kernel))\n",
    "print(\"Mejor Gamma: {}\".format(tuning_svm.best_estimator_.gamma))\n",
    "final_svm = SVC(C=tuning_svm.best_estimator_.C, \n",
    "                kernel=tuning_svm.best_estimator_.kernel, \n",
    "                gamma=tuning_svm.best_estimator_.gamma, probability=True)\n",
    "final_svm.fit(X_train, y_train)\n",
    "svm_final_metrics = get_metrics(final_svm)\n",
    "print(svm_final_metrics)\n",
    "save_model(final_svm, \"SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor accuracy: 0.9062863795110594\n",
      "Mejor K: 5\n",
      "Mejor algoritmo de búsqueda de vecinos: kd_tree\n",
      "{'accuracy': 0.921644685802948, 'precision': 0.927710843373494, 'recall': 0.8, 'f1-score': 0.8591352859135286}\n"
     ]
    }
   ],
   "source": [
    "knn_params = {\"n_neighbors\": np.arange(5) + 1, \"algorithm\": [\"kd_tree\", \"ball_tree\"]}\n",
    "tuning_knn = estimator_grid_search(KNeighborsClassifier(), knn_params)\n",
    "print(\"Mejor K: {}\".format(tuning_knn.best_estimator_.n_neighbors)) \n",
    "print(\"Mejor algoritmo de búsqueda de vecinos: {}\".format(tuning_knn.best_estimator_.algorithm))\n",
    "final_knn = KNeighborsClassifier(n_neighbors=tuning_knn.best_estimator_.n_neighbors, \n",
    "                                 algorithm=tuning_knn.best_estimator_.algorithm)\n",
    "final_knn.fit(X_train, y_train)\n",
    "knn_final_metrics = get_metrics(final_knn)\n",
    "print(knn_final_metrics)\n",
    "save_model(final_knn, \"KNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor accuracy: 0.8610787737679473\n",
      "Mejor cantidad de árboles: 9\n",
      "Mejor criterio de división: entropy\n",
      "Mejor cantidad máxima de features: None\n",
      "{'accuracy': 0.86966640806827, 'precision': 0.8258258258258259, 'recall': 0.7142857142857143, 'f1-score': 0.7660167130919221}\n"
     ]
    }
   ],
   "source": [
    "rf_params = {\n",
    "    \"n_estimators\": np.arange(10) + 1, \n",
    "    \"criterion\": [\"gini\", \"entropy\"], \n",
    "    \"max_features\": [\"sqrt\", \"log2\", None]}\n",
    "tuning_rf = estimator_grid_search(RandomForestClassifier(), rf_params)\n",
    "print(\"Mejor cantidad de árboles: {}\".format(tuning_rf.best_estimator_.n_estimators))\n",
    "print(\"Mejor criterio de división: {}\".format(tuning_rf.best_estimator_.criterion))\n",
    "print(\"Mejor cantidad máxima de features: {}\".format(tuning_rf.best_estimator_.max_features))\n",
    "final_rf = RandomForestClassifier(n_estimators=tuning_rf.best_estimator_.n_estimators,\n",
    "                                 criterion=tuning_rf.best_estimator_.criterion,\n",
    "                                 max_features=tuning_rf.best_estimator_.max_features)\n",
    "final_rf.fit(X_train, y_train)\n",
    "rf_final_metrics = get_metrics(final_rf)\n",
    "print(rf_final_metrics)\n",
    "save_model(final_rf, \"RF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(clf, test_set):\n",
    "    return clf.predict_proba(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pequeña prueba de modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.77777778 0.22222222]]\n"
     ]
    }
   ],
   "source": [
    "test_words = [\"sex\"]\n",
    "test_words_matrix = np.empty((0, 300))\n",
    "for word in test_words:\n",
    "    test_words_matrix = np.append(test_words_matrix, [vec(word)], axis=0)\n",
    "print(model_predict(final_rf, test_words_matrix))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_knn.predict(test_words_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
