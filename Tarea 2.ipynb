{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tarea 2: análisis de sentimientos usando aprendizaje supervisado\n",
    "\n",
    "##### Por: Daniela Flores Villanueva \n",
    "\n",
    "## Sobre las librerías"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En primer lugar, se cargan todas las librerías empleadas en esta tarea:\n",
    "- `pandas`: librería utilizada para cargar la matriz GloVe y posteriormente, para confeccionar un `DataFrame` con palabras y su sentimiento asociado.\n",
    "- `csv`: utilizada para cargar los vectores, según lo dispuesto en [este](https://stackoverflow.com/questions/37793118/load-pretrained-glove-vectors-in-python) sitio.\n",
    "- `nltk`: se usó para cargar el *opinion lexicon* descrito en el enunciado de manera conveniente. Se probó además sus métodos para realizar lematización, proceso que consiste en, dada una palabra flexionada (en forma plural, verbo conjugado, etc), encontrar su lema, es decir, algo similar a la forma en que sería encontrada en un diccionario.\n",
    "- `string`: para obtener signos de puntuación, con el objeto de pre-procesar oraciones.\n",
    "- `sklearn`: se utilizaron sus implementaciones de *Support Vector Machines*, *K-Nearest Neighbor* y *Random Forest* para construir clasificadores para el sentimiento de palabras y posteriormente oraciones. Se aprovechó también `train_test_split` para separar los datos en conjuntos de entrenamiento y de prueba, `GridSearchCV` para la calibración de hiperparámetros y los métodos `accuracy_score`, `precision_score`, `recall_score` y `f1-score` para cuantificar el desempeño de los modelos. Finalmente `joblib` se utiliza para serializar y guardar los modelos entrenados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/DanielaFlores/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package opinion_lexicon to\n",
      "[nltk_data]     /Users/DanielaFlores/nltk_data...\n",
      "[nltk_data]   Package opinion_lexicon is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/DanielaFlores/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import csv\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "import numpy as np\n",
    "import nltk\n",
    "nltk.download(\"stopwords\")\n",
    "nltk.download(\"opinion_lexicon\")\n",
    "nltk.download('punkt')\n",
    "from nltk.corpus import opinion_lexicon\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from string import punctuation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from sklearn.externals import joblib\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "from countryinfo import countries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(formatter={'float_kind':'{:f}'.format})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "GLOVE_PATH = \"./glove.42B.300d.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "NON_WORDS = list(punctuation)\n",
    "NON_WORDS.extend(map(str, range(10)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ENGLISH_STOPWORDS = stopwords.words(\"english\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La forma de cargar el opinion lexicon es la descrita en la [documentación](http://www.nltk.org/api/nltk.corpus.reader.html?highlight=wordnet) de NLTK."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = opinion_lexicon.positive()\n",
    "negative_data = opinion_lexicon.negative()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Con el fin de tener una representación más general de las palabras, se convierte cada vocablo en las listas anteriores a minúsculas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data = [word.lower() for word in positive_data]\n",
    "negative_data = [word.lower() for word in negative_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se carga la matriz de *embeddings*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "glove_matrix = pd.read_csv(GLOVE_PATH, sep=\" \", index_col=0, header=None, quoting=csv.QUOTE_NONE, na_values=None, keep_default_na=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el mismo recurso para cargar la matriz en memoria (adjunto en la descripción de las librerías utilizadas), se menciona una función que, dada una palabra, retorna su *embedding*. Esta función es `vec(w)`, definida a continuación:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec(w): \n",
    "    return glove_matrix.loc[w].as_matrix()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Breve análisis exploratorio\n",
    "\n",
    "No se puede empezar a trabajar sin tener al menos una noción básica de la composición de la matriz y de las palabras con etiqueta. Por esta razón, en primer lugar cabe preguntarse cuántas palabras hay en la matriz de *embeddings*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1917494\n"
     ]
    }
   ],
   "source": [
    "glove_words = set(glove_matrix.index.tolist())\n",
    "print(len(glove_words))\n",
    "pickle.dump(glove_words, open(\"glove_words.pkl\", \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podría ocurrir que no todas las palabras que tenemos etiquetadas según su polaridad tengan su representación vectorial en GloVe, por lo que conviene realizar el siguiente análisis:\n",
    "Dada la unión entre las palabras de polaridad positiva y las de polaridad negativa, se revisa si están o no en la matriz. De no estar, se agregan a la lista `missing_words`, para decidir qué hacer con ellas futuramente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161\n"
     ]
    }
   ],
   "source": [
    "missing_words = set()\n",
    "for word in set(positive_data).union(set(negative_data)):\n",
    "    if word not in glove_words:\n",
    "        missing_words.add(word)\n",
    "print(len(missing_words))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Es posible notar que hay 161 palabras del *opinion lexicon* para las que no se tiene una representación vectorial. Antes de decidir qué hacer con estas palabras faltantes, conviene preguntarse si existen palabras que estén etiquetadas tanto de polaridad positiva como de polaridad negativa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'envious', 'enviousness', 'enviously'}\n"
     ]
    }
   ],
   "source": [
    "wrong_tag = set(positive_data).intersection(negative_data)\n",
    "print(wrong_tag)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, se evidencia que tres palabras relacionadas a la envidia parecen estar mal etiquetadas, pues no deberían aparecer entre las palabras positivas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A continuación, se procede a realizar lematización de los vocablos positivos y negativos, para ver si existe algún cambio en la cantidad palabras sin *embedding* tras este cambio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data_lemmatized = {nltk.stem.WordNetLemmatizer().lemmatize(word) for word in positive_data}\n",
    "negative_data_lemmatized = {nltk.stem.WordNetLemmatizer().lemmatize(word) for word in negative_data}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160\n"
     ]
    }
   ],
   "source": [
    "missing_words_lemmatized = set()\n",
    "for word in positive_data_lemmatized.union(set(negative_data_lemmatized)):\n",
    "    if word not in glove_words:\n",
    "        missing_words_lemmatized.add(word)\n",
    "print(len(missing_words_lemmatized))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'envious', 'plea', 'enviousness', 'enviously'}\n"
     ]
    }
   ],
   "source": [
    "wrong_tag_lemmatized = set(positive_data_lemmatized).intersection(negative_data_lemmatized)\n",
    "print(wrong_tag_lemmatized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_data_lemmatized = positive_data_lemmatized - wrong_tag_lemmatized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al lematizar, es posible notar que hay una palabra menos que no tiene representación vectorial. En la siguiente celda, podrá evidenciarse qué palabra es la que hace la diferencia:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'spoilages'}\n"
     ]
    }
   ],
   "source": [
    "print(missing_words - missing_words_lemmatized)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocesamiento de los datos\n",
    "\n",
    "En las siguientes líneas, se iterará por sobre todas las palabras en los conjuntos de palabras reducidas a su lema (`positive_data_lemmatized` y `negative_data_lemmatized`), con el fin de encontrar su vector en la matriz de *embeddings* GloVe. Estos vectores se guardarán en las matrices `positive_matrix` y `negative_matrix`, que contendrán, respectivamente, las palabras de sentimiento positivo y negativo.\n",
    "\n",
    "Es importante destacar el trato que reciben las palabras para las que no se dispone una representación vectorial. Tras [investigar](https://github.com/stanfordnlp/GloVe/search?utf8=%E2%9C%93&q=unk&type=), se descubrió que GloVe incluye un vector estándar para las palabras sin *word embedding*, el que en esta tarea puede obtenerse con `vec(\"unk\")`. Así, a todas las palabras del *opinion lexicon* que no tienen *embedding* en GloVe, se les asigna el vector antes mencionado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_matrix = np.empty((0, 300))\n",
    "positive_data_lemmatized = list(positive_data_lemmatized)\n",
    "for word in positive_data_lemmatized:\n",
    "    if word in glove_words:\n",
    "        positive_matrix = np.append(positive_matrix, [vec(word)], axis=0)\n",
    "    else:\n",
    "        positive_matrix = np.append(positive_matrix, [vec(\"unk\")], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_matrix = np.empty((0, 300))\n",
    "negative_data_lemmatized = list(negative_data_lemmatized)\n",
    "for word in negative_data_lemmatized:\n",
    "    if word in glove_words:\n",
    "        negative_matrix = np.append(negative_matrix, [vec(word)], axis=0)\n",
    "    else:\n",
    "        negative_matrix = np.append(negative_matrix, [vec(\"unk\")], axis=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se observa que las matrices recién formadas efectivamente contienen todas las representaciones vectoriales que corresponden:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1951 (1951, 300)\n",
      "4492 (4492, 300)\n"
     ]
    }
   ],
   "source": [
    "print(len(positive_data_lemmatized), positive_matrix.shape)\n",
    "print(len(negative_data_lemmatized), negative_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se definen las etiquetas para cada ejemplo en el *data set*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_labels = [1] * positive_matrix.shape[0]\n",
    "negative_labels = [0] * negative_matrix.shape[0]\n",
    "all_labels = positive_labels + negative_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se concatenan las matrices `positive_matrix` y `negative_matrix`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(6443, 300)\n"
     ]
    }
   ],
   "source": [
    "concatenated_matrix = np.concatenate([positive_matrix, negative_matrix], axis=0)\n",
    "print(concatenated_matrix.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clasificación de sentimiento de palabras\n",
    "\n",
    "### Separación entre entrenamiento y *testing*\n",
    "\n",
    "Con el objeto de hacer una correcta clasificación, se separan los datos en entrenamiento y *testing*. En particular, se utiliza el 80% de los datos para entrenar los modelos y 20% para probarlos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_words = positive_data_lemmatized + negative_data_lemmatized\n",
    "X_train, X_test, y_train, y_test, words_train, words_test = train_test_split(concatenated_matrix, all_labels, all_words, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calibración de híper-parámetros\n",
    "\n",
    "Para mejorar el desempeño de los modelos, se define la siguiente función para calibrar los híper-parámetros correspondientes. Esto se hace posible gracias a `GridSearchCV`, que recibe un modelo, los datos de entrenamiento, los híper-parámetros a testear y la cantidad de *folds* en que se separarán los datos (de acuerdo a la documentación, los datos se separán de forma que se conserven las proporciones originales de las clases). Con esta información, `GridSearchCV` probará todas las combinaciones de híper-parámetros y se realizará validación cruzada con ellas. Finalmente se retornará la combinación de híper-parámetros que haya obtenido mejor `cross_val_score`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def estimator_grid_search(estimator, parameters):\n",
    "    clf = GridSearchCV(estimator=estimator, param_grid=parameters, n_jobs=-1, cv=5)\n",
    "    clf.fit(X_train, y_train)   \n",
    "    print(\"Mejor accuracy: {}\".format(clf.best_score_))\n",
    "    return clf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Medición del desempeño de los modelos\n",
    "\n",
    "Para evaluar los modelos, es necesario cuantificar su desempeño. Por esta razón, se define la función `get_metrics` que dado un conjunto de entrenamiento con sus respectivas etiquetas, permite obtener un diccionario con las principales métricas para tareas de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_metrics(clf, X_test, y_test):\n",
    "    metrics = {}\n",
    "    predictions = clf.predict(X_test)\n",
    "    metrics[\"accuracy\"] = accuracy_score(y_test, predictions)\n",
    "    metrics[\"precision\"] = precision_score(y_test, predictions)\n",
    "    metrics[\"recall\"] = recall_score(y_test, predictions)\n",
    "    metrics[\"f1-score\"] = f1_score(y_test, predictions)\n",
    "    return metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Guardar modelos\n",
    "\n",
    "La función `save_model` permite guardar los clasificadores entrenados en esta tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_model(clf, clf_name):\n",
    "    joblib.dump(clf, \"best_{}.pkl\".format(clf_name))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM\n",
    "\n",
    "\n",
    "El primer modelo entrenado en esta tarea corresponde a *Support Vector Machines*. Para ajustarlo, primero se realiza la calibración de híper-parámetros. Una vez encontrada la mejor combinación de ellos, se entrena el clasificador con ellos y los datos de entrenamiento. Finalmente, se obtienen las métricas antes descritas sobre el set de *testing*. Como los datos de este conjunto no han sido vistos por el modelo, puede asumirse que las métricas son una buena muestra de la calidad del clasificador."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mejor accuracy: 0.936942180830423\n",
      "Mejor C: 10\n",
      "Mejor kernel: sigmoid\n",
      "Mejor Gamma: 0.001\n",
      "{'accuracy': 0.948021722265322, 'precision': 0.9491525423728814, 'recall': 0.8727272727272727, 'f1-score': 0.9093369418132612}\n"
     ]
    }
   ],
   "source": [
    "svm_params = {\n",
    "    \"C\": [1, 10],\n",
    "    \"gamma\": [0.001, 0.0001],\n",
    "    \"kernel\": [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n",
    "}\n",
    "tuning_svm = estimator_grid_search(SVC(), svm_params)\n",
    "print(\"Mejor C: {}\".format(tuning_svm.best_estimator_.C))\n",
    "print(\"Mejor kernel: {}\".format(tuning_svm.best_estimator_.kernel))\n",
    "print(\"Mejor Gamma: {}\".format(tuning_svm.best_estimator_.gamma))\n",
    "final_svm = SVC(C=tuning_svm.best_estimator_.C, \n",
    "                kernel=tuning_svm.best_estimator_.kernel, \n",
    "                gamma=tuning_svm.best_estimator_.gamma, probability=True)\n",
    "final_svm.fit(X_train, y_train)\n",
    "svm_final_metrics = get_metrics(final_svm, X_test, y_test)\n",
    "print(svm_final_metrics)\n",
    "save_model(final_svm, \"SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KNN\n",
    "\n",
    "Se realizan los mismos pasos que en el caso del SVM, pero ahora para entrenar un clasificador KNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knn_params = {\"n_neighbors\": np.arange(5) + 1, \"algorithm\": [\"kd_tree\", \"ball_tree\"]}\n",
    "tuning_knn = estimator_grid_search(KNeighborsClassifier(), knn_params)\n",
    "print(\"Mejor K: {}\".format(tuning_knn.best_estimator_.n_neighbors)) \n",
    "print(\"Mejor algoritmo de búsqueda de vecinos: {}\".format(tuning_knn.best_estimator_.algorithm))\n",
    "final_knn = KNeighborsClassifier(n_neighbors=tuning_knn.best_estimator_.n_neighbors, \n",
    "                                 algorithm=tuning_knn.best_estimator_.algorithm)\n",
    "final_knn.fit(X_train, y_train)\n",
    "knn_final_metrics = get_metrics(final_knn, X_test, y_test)\n",
    "print(knn_final_metrics)\n",
    "save_model(final_knn, \"KNN\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random Forest\n",
    "\n",
    "Finalmente, se evaluó el desempeño de un ensamble de árboles de decisión sobre esta tarea de clasificación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_params = {\n",
    "    \"n_estimators\": np.arange(10) + 1, \n",
    "    \"criterion\": [\"gini\", \"entropy\"], \n",
    "    \"max_features\": [\"sqrt\", \"log2\", None]}\n",
    "tuning_rf = estimator_grid_search(RandomForestClassifier(), rf_params)\n",
    "print(\"Mejor cantidad de árboles: {}\".format(tuning_rf.best_estimator_.n_estimators))\n",
    "print(\"Mejor criterio de división: {}\".format(tuning_rf.best_estimator_.criterion))\n",
    "print(\"Mejor cantidad máxima de features: {}\".format(tuning_rf.best_estimator_.max_features))\n",
    "final_rf = RandomForestClassifier(n_estimators=tuning_rf.best_estimator_.n_estimators,\n",
    "                                 criterion=tuning_rf.best_estimator_.criterion,\n",
    "                                 max_features=tuning_rf.best_estimator_.max_features)\n",
    "final_rf.fit(X_train, y_train)\n",
    "rf_final_metrics = get_metrics(final_rf, X_test, y_test)\n",
    "print(rf_final_metrics)\n",
    "save_model(final_rf, \"RF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el enunciado de la tarea, se pide que los modelos tengan salida continua. Es por eso que se define la función `model_predict`, que retorna la probabilidad de pertenencia a cada una de las clases (positivo/negativo)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_predict(clf, test_set):\n",
    "    return clf.predict_proba(test_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparación de clasificadores\n",
    "\n",
    "A continuación, se comparan los clasificadores según las métricas anteriormente descritas:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = np.arange(4)\n",
    "r2 = [x + 0.25 for x in r1]\n",
    "r3 = [x + 0.25 for x in r2]\n",
    "\n",
    "labels = [k for k, v in svm_final_metrics.items()]\n",
    "svm_values = [v for k, v in svm_final_metrics.items()]\n",
    "knn_values = [v for k, v in knn_final_metrics.items()]\n",
    "rf_values = [v for k, v in rf_final_metrics.items()]\n",
    "\n",
    "plt.xticks([r + 0.25 for r in range(4)], labels)\n",
    "ax = plt.subplot(111)\n",
    "ax.bar(r1, svm_values,width=0.25,color='b', align='center')\n",
    "ax.bar(r2, knn_values, width=0.25,color='g', align='center')\n",
    "ax.bar(r3, rf_values, width=0.25, color='r', align='center')\n",
    "plt.legend([\"SVM\", \"KNN\", \"RF\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comparar las salidas continuas con las verdaderas etiquetas de los datos, se crean los siguientes `DataFrame`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "svm_df = pd.DataFrame(model_predict(final_svm,  X_test), columns=[\"negative_probability\", \"positive_probability\"])\n",
    "knn_df = pd.DataFrame(model_predict(final_knn, X_test), columns=[\"negative_probability\", \"positive_probability\"])\n",
    "rf_df = pd.DataFrame(model_predict(final_rf, X_test), columns=[\"negative_probability\", \"positive_probability\"])\n",
    "svm_df[\"word\"] = words_test\n",
    "knn_df[\"word\"] = words_test\n",
    "rf_df[\"word\"] = words_test\n",
    "svm_df[\"true_label\"] = y_test\n",
    "knn_df[\"true_label\"] = y_test\n",
    "rf_df[\"true_label\"] = y_test\n",
    "\n",
    "svm_df = svm_df[[\"word\", \"negative_probability\", \"positive_probability\", \"true_label\"]]\n",
    "knn_df = knn_df[[\"word\", \"negative_probability\", \"positive_probability\", \"true_label\"]]\n",
    "rf_df = rf_df[[\"word\", \"negative_probability\", \"positive_probability\", \"true_label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_words = svm_df.sample(5)[\"word\"].tolist()\n",
    "\n",
    "display(svm_df[svm_df[\"word\"].isin(random_words)])\n",
    "display(knn_df[knn_df[\"word\"].isin(random_words)])\n",
    "display(rf_df[rf_df[\"word\"].isin(random_words)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicción de oraciones\n",
    "\n",
    "En esta sección se detalla lo necesario para extender el poder de los modelos a la clasificación de oraciones de largo arbitrario.\n",
    "### Preprocesamiento de las oraciones\n",
    "\n",
    "Con el fin de eliminar las partes de las oraciones que no aportan demasiado a su significado, se realiza lo siguiente dentro de la función `tokenize_lemmatize`:\n",
    "- Se convierte la oración recibida a minúsculas.\n",
    "- Se eliminan los caracteres que no corresponden a letras (signos de puntuación y números).\n",
    "- Se separa la oración en *tokens*.\n",
    "- Se remueven las *stopwords*, palabras comunes que no contribuyen al significado de una oración.\n",
    "- Lematización, tal como se explicó en una sección anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_lemmatize(sentence):\n",
    "    sentence = sentence.lower()\n",
    "    words_sentence = ''.join([c for c in sentence if c not in NON_WORDS])\n",
    "    tokenized_sentence = word_tokenize(words_sentence)\n",
    "    removed_stopwords = [word for word in tokenized_sentence if word not in ENGLISH_STOPWORDS]\n",
    "    lemmatized_sentence = [nltk.stem.WordNetLemmatizer().lemmatize(word) for word in removed_stopwords]\n",
    "    return tokenized_sentence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La obtención de la representación vectorial es similar a la utilizada para las palabras. En el caso de las oraciones, se obtiene el vector de cada palabra en la oración y se guarda en `sentence_matrix`. De no contar con el *embedding*, se utiliza el vector de \"unk\". Posteriormente, se promedian todas las coordenadas de los vectores en `sentence_matrix` y se guarda en `mean_sentence_vector`. Este será el vector que represente a una oración."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorize_sentence(sentence):\n",
    "    sentence_matrix = np.empty((0, 300))\n",
    "    for word in sentence:\n",
    "        if word in glove_words:\n",
    "            sentence_matrix = np.append(sentence_matrix, [vec(word)], axis=0)\n",
    "        else:\n",
    "            sentence_matrix = np.append(sentence_matrix, [vec(\"unk\")], axis=0)\n",
    "    mean_sentence_vector = sentence_matrix.mean(0)\n",
    "    if np.isnan(np.sum(mean_sentence_vector)):\n",
    "        mean_sentence_vector = vec(\"unk\")\n",
    "    return mean_sentence_vector"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La siguiente función ejecuta las dos funciones anteriores y retorna la representación vectorial de la oración entregada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_sentence(sentence):\n",
    "    tokenized = tokenize_lemmatize(sentence)\n",
    "    vectorized = vectorize_sentence(tokenized)\n",
    "    return vectorized"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluación de los modelos sobre frases reales\n",
    "\n",
    "Para evaluar el desempeño del modelo, se utiliza el *data set* [Sentiment Labelled Sentences](https://archive.ics.uci.edu/ml/datasets/Sentiment+Labelled+Sentences). Las siguientes líneas cargan y pre-procesan estos nuevos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "amazon = pd.read_csv(\"amazon_cells_labelled.txt\", sep=\"\\t\", names=[\"sentence\", \"polarity\"])\n",
    "imdb = pd.read_csv(\"imdb_labelled.txt\", sep=\"\\t\", names=[\"sentence\", \"polarity\"])\n",
    "yelp = pd.read_csv(\"yelp_labelled.txt\", sep=\"\\t\", names=[\"sentence\", \"polarity\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_sentences = pd.concat([amazon, imdb, yelp])\n",
    "all_sentences.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_sentence_test = all_sentences[\"sentence\"].tolist()\n",
    "y_sentence_test = all_sentences[\"polarity\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_sentences = list(map(prepare_sentence, X_sentence_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo anterior permite obtener las siguientes métricas: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(get_metrics(final_svm, vectorized_sentences, y_sentence_test))\n",
    "print(get_metrics(final_knn, vectorized_sentences, y_sentence_test))\n",
    "print(get_metrics(final_rf, vectorized_sentences, y_sentence_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reentrenamiento de los modelos\n",
    "\n",
    "Se observa que el desempeño no es tan bueno como el que se tiene para la predicción de sentimiento de palabras. Por esta razón, se reentrenan los modelos anteriores con los datos para palabras y el 80% de los nuevos datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_concatenated_matrix = np.concatenate([positive_matrix, negative_matrix, vectorized_sentences], axis=0)\n",
    "new_all_labels = positive_labels + negative_labels + y_sentence_test.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "words_and_sentences = all_words + X_sentence_test\n",
    "new_X_train, new_X_test, new_y_train, new_y_test, new_words_train, new_words_test  = train_test_split(new_concatenated_matrix, new_all_labels, words_and_sentences, test_size=0.2, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_svm = SVC(C=tuning_svm.best_estimator_.C, \n",
    "                kernel=tuning_svm.best_estimator_.kernel, \n",
    "                gamma=tuning_svm.best_estimator_.gamma, probability=True)\n",
    "new_knn = KNeighborsClassifier(n_neighbors=tuning_knn.best_estimator_.n_neighbors, \n",
    "                                 algorithm=tuning_knn.best_estimator_.algorithm)\n",
    "new_rf = RandomForestClassifier(n_estimators=tuning_rf.best_estimator_.n_estimators,\n",
    "                                 criterion=tuning_rf.best_estimator_.criterion,\n",
    "                                 max_features=tuning_rf.best_estimator_.max_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_svm.fit(new_X_train, new_y_train)\n",
    "new_knn.fit(new_X_train, new_y_train)\n",
    "new_rf.fit(new_X_train, new_y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Así, se obtienen las siguientes métricas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_svm_metrics = get_metrics(new_svm, new_X_test, new_y_test)\n",
    "new_knn_metrics = get_metrics(new_knn, new_X_test, new_y_test)\n",
    "new_rf_metrics = get_metrics(new_rf, new_X_test, new_y_test)\n",
    "print(new_svm_metrics)\n",
    "print(new_knn_metrics)\n",
    "print(new_rf_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r1 = np.arange(4)\n",
    "r2 = [x + 0.25 for x in r1]\n",
    "r3 = [x + 0.25 for x in r2]\n",
    "\n",
    "labels = [k for k, v in new_svm_metrics.items()]\n",
    "svm_values = [v for k, v in new_svm_metrics.items()]\n",
    "knn_values = [v for k, v in new_knn_metrics.items()]\n",
    "rf_values = [v for k, v in new_rf_metrics.items()]\n",
    "plt.xticks([r + 0.25 for r in range(4)], labels)\n",
    "ax = plt.subplot(111)\n",
    "ax.bar(r1, svm_values,width=0.25,color='b', align='center')\n",
    "ax.bar(r2, knn_values, width=0.25,color='g', align='center')\n",
    "ax.bar(r3, rf_values, width=0.25, color='r', align='center')\n",
    "plt.legend([\"SVM\", \"KNN\", \"RF\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Los siguientes `DataFrames` permiten visualizar las salidas continuas y la verdadera etiqueta de los datos de *testing*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_svm_df = pd.DataFrame(model_predict(new_svm,  new_X_test), columns=[\"negative_probability\", \"positive_probability\"])\n",
    "new_knn_df = pd.DataFrame(model_predict(new_knn, new_X_test), columns=[\"negative_probability\", \"positive_probability\"])\n",
    "new_rf_df = pd.DataFrame(model_predict(new_rf, new_X_test), columns=[\"negative_probability\", \"positive_probability\"])\n",
    "new_svm_df[\"word_sentence\"] = new_words_test\n",
    "new_knn_df[\"word_sentence\"] = new_words_test\n",
    "new_rf_df[\"word_sentence\"] = new_words_test\n",
    "new_svm_df[\"true_label\"] = new_y_test\n",
    "new_knn_df[\"true_label\"] = new_y_test\n",
    "new_rf_df[\"true_label\"] = new_y_test\n",
    "new_svm_df = new_svm_df[[\"word_sentence\", \"negative_probability\", \"positive_probability\", \"true_label\"]]\n",
    "new_knn_df = new_knn_df[[\"word_sentence\", \"negative_probability\", \"positive_probability\", \"true_label\"]]\n",
    "new_rf_df = new_rf_df[[\"word_sentence\", \"negative_probability\", \"positive_probability\", \"true_label\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_words_sentences = new_svm_df.sample(5)[\"word_sentence\"].tolist()\n",
    "\n",
    "display(new_svm_df[new_svm_df[\"word_sentence\"].isin(random_words_sentences)])\n",
    "display(new_knn_df[new_knn_df[\"word_sentence\"].isin(random_words_sentences)])\n",
    "display(new_rf_df[new_rf_df[\"word_sentence\"].isin(random_words_sentences)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finalmente, se serializan los modelos para referencias futuras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model(new_svm, \"sentences_SVM\")\n",
    "save_model(new_knn, \"sentences_KNN\")\n",
    "save_model(new_rf, \"sentences_RF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluación y caracterización de prejuicios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "american_countries = [country[\"name\"] for country in countries if \"America\" in country[\"continent\"]]\n",
    "european_countries = [country[\"name\"] for country in countries if \"Europe\" in country[\"continent\"]]\n",
    "african_countries = [country[\"name\"] for country in countries if \"Africa\" in country[\"continent\"]]\n",
    "asian_countries = [country[\"name\"] for country in countries if \"Asia\" in country[\"continent\"]]\n",
    "oceanian_countries = [country[\"name\"] for country in countries if \"Oceania\" in country[\"continent\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_svm = joblib.load(\"best_sentences_SVM.pkl\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://gist.github.com/pamelafox/986163\n",
    "https://github.com/knowitall/chunkedextractor/blob/master/src/main/resources/edu/knowitall/chunkedextractor/demonyms.csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "demonyms = pd.read_csv(\"demonyms.csv\", header=None)\n",
    "demonyms.columns = [\"demonym\", \"place\"]\n",
    "american_demonyms = demonyms[demonyms[\"place\"].isin(american_countries)][\"demonym\"].tolist()\n",
    "european_demonyms = demonyms[demonyms[\"place\"].isin(european_countries)][\"demonym\"].tolist()\n",
    "african_demonyms = demonyms[demonyms[\"place\"].isin(african_countries)][\"demonym\"].tolist()\n",
    "asian_demonyms = demonyms[demonyms[\"place\"].isin(asian_countries)][\"demonym\"].tolist()\n",
    "oceanian_demonyms = demonyms[demonyms[\"place\"].isin(oceanian_countries)][\"demonym\"].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "american_sentence = list({\"Let's go get {} food\".format(demonym) for demonym in american_demonyms})\n",
    "european_sentence = list({\"Let's go get {} food\".format(demonym) for demonym in european_demonyms})\n",
    "african_sentence = list({\"Let's go get {} food\".format(demonym) for demonym in european_demonyms})\n",
    "asian_sentence = list({\"Let's go get {} food\".format(demonym) for demonym in asian_demonyms})\n",
    "oceanian_sentence = list({\"Let's go get {} food\".format(demonym) for demonym in oceanian_demonyms})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorized_american = list(map(prepare_sentence, american_sentence))\n",
    "vectorized_european = list(map(prepare_sentence, european_sentence))\n",
    "vectorized_african = list(map(prepare_sentence, african_sentence))\n",
    "vectorized_asian = list(map(prepare_sentence, asian_sentence))\n",
    "vectorized_oceanian = list(map(prepare_sentence, oceanian_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "american_predictions = pd.DataFrame(model_predict(test_svm, vectorized_american), columns=[\"negative_probability\", \"positive_probability\"])\n",
    "european_predictions = pd.DataFrame(model_predict(test_svm, vectorized_european), columns=[\"negative_probability\", \"positive_probability\"])\n",
    "african_predictions = pd.DataFrame(model_predict(test_svm, vectorized_african), columns=[\"negative_probability\", \"positive_probability\"])\n",
    "asian_predictions = pd.DataFrame(model_predict(test_svm, vectorized_asian), columns=[\"negative_probability\", \"positive_probability\"])\n",
    "oceanian_predictions = pd.DataFrame(model_predict(test_svm, vectorized_oceanian), columns=[\"negative_probability\", \"positive_probability\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "american_predictions_mean = american_predictions.mean().tolist()\n",
    "european_predictions_mean = european_predicions.mean().tolist()\n",
    "african_predictions_mean = african_predictions.mean().tolist()\n",
    "asian_predictions_mean = asian_predictions.mean().tolist()\n",
    "oceanian_predictions_mean = oceanian_predictions.mean().tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "continents_mean = {\n",
    "    \"American\": american_predictions_mean,\n",
    "    \"European\": european_predictions_mean,\n",
    "    \"African\": african_predictions_mean,\n",
    "    \"Asian\": asian_predictions_mean,\n",
    "    \"Oceanian\": oceanian_predictions_mean,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "negative_means = {k: v[0] for k, v in continents_mean.items()}\n",
    "positive_means = {k: v[1] for k, v in continents_mean.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(negative_means.keys(), negative_means.values())\n",
    "plt.title(\"Probabilidad promedio de sentimiento negativo según continente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.bar(positive_means.keys(), positive_means.values())\n",
    "plt.title(\"Probabilidad promedio de sentimiento positivo según continente\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
